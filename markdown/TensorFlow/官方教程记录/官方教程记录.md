### 数据流图相关问题

#### tensorflow使用图的原因

为了用python实现高效的数值计算，我们通常会使用函数库，比如NumPy，<u>会把类似矩阵乘法这样的复杂运算使用其他外部语言实现</u>。不幸的是，从外部计算切换回Python的每一个操作，仍然是一个很大的开销。如果你用GPU来进行外部计算，这样的开销会更大。用分布式的计算方式，也会花费更多的资源用来传输数据。

TensorFlow也把复杂的计算放在python之外完成，但是为了避免前面说的那些开销，它做了进一步完善。<u>Tensorflow不单独地运行单一的复杂计算，而是让我们可以先**用图描述**一系列可交互的计算操作，然后全部一起在Python之外运行</u>。（这样类似的运行方式，可以在不少的机器学习库中看到。）

**因此Python代码的目的**是用来构建这个可以在外部运行的计算图，以及安排计算图的哪一部分应该被运行。



#### 为什么`c = tf.matmul(a, b)` 不立即执行矩阵相乘？

在 TensorFlow 的 Python API 中, `a`, `b`, and `c` 都是 `Tensor`对象. **一个 `Tensor` 对象是一个操作(operation)结果的字符别名,它实际上并不储存操作(operation)输出结果的值**。 TensorFlow 鼓励用户去建立复杂的表达式（如整个神经网络及其梯度）来形成 data flow graph 。 然后你可以将整个 data flow graph 的计算过程交给一个 TensorFlow 的 [`Session`](http://www.tensorfly.cn/tfdoc/api_docs/python/client.html#Session), 此 `Session` 可以运行整个计算过程，比起操作(operations)一条一条的执行效率高的多。

#### 运行TensorFlow的InteractiveSession

Tensorflow依赖于一个高效的C++后端来进行计算。与后端的这个连接叫做session。**一般而言，使用TensorFlow程序的流程是先创建一个图，然后在session中启动它。**

这里，我们使用更加方便的`InteractiveSession`类。通过它，你可以更加灵活地构建你的代码。**它能让你在运行图的时候，插入一些[计算图](http://www.tensorfly.cn/tfdoc/get_started/basic_usage.html#the-computation-graph)，**这些计算图是由某些操作(operations)构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用IPython。如果你没有使用`InteractiveSession`，那么你需要在启动session之前构建整个计算图，然后[启动该计算图](http://www.tensorfly.cn/tfdoc/get_started/basic_usage.html#launching-the-graph-in-a-session)。

```
import tensorflow as tf
sess = tf.InteractiveSession()
```



#### Dropout

为了减少过拟合，我们在输出层之前加入dropout。我们用一个`placeholder`来代表一个神经元的输出在dropout中保持不变的概率。这样我们**可以在训练过程中启用dropout，在测试过程中关闭dropout。** 



#### global step

我们实例化一个[`tf.train.GradientDescentOptimizer`](http://www.tensorfly.cn/tfdoc/api_docs/python/train.html#GradientDescentOptimizer)，负责按照所要求的学习效率（learning rate）应用梯度下降法（gradients）。

```
optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)
```

之后，我们生成一个变量用于**保存全局训练步骤**（global training step）的数值，并**使用[`minimize()`](http://www.tensorfly.cn/tfdoc/api_docs/python/train.html#Optimizer.minimize)函数更新增加全局步骤**、系统中的三角权重（triangle weights）的操作。

根据惯例，这个操作被称为 `train_op`，是TensorFlow会话（session）诱发一个完整训练步骤所必须运行的操作（见下文）。

```
global_step = tf.Variable(0, name='global_step', trainable=False)
train_op = optimizer.minimize(loss, global_step=global_step)
```



### 计算图

#### 构建图

构建图的第一步, 是创建源 op (source op  operation 的缩写). 源 op 不需要任何输入, 例如 `常量 (Constant)`. 源 op 的输出被传递给其它 op 做运算.

**Python 库中, op 构造器的返回值代表被构造出的 op 的输出, 这些返回值可以传递给其它 op 构造器作为输入.**

TensorFlow Python 库有一个**默认图 (default graph)**, op 构造器可以为其增加节点. 这个默认图对 许多程序来说已经足够用了. 阅读 [Graph 类](http://www.tensorfly.cn/tfdoc/api_docs/python/framework.html#Graph) 文档 来了解如何管理多个图.

```python
import tensorflow as tf

# 创建一个常量 op, 产生一个 1x2 矩阵. 这个 op 被作为一个节点
# 加到默认图中.

# 构造器的返回值代表该常量 op 的返回值.
matrix1 = tf.constant([[3., 3.]])

# 创建另外一个常量 op, 产生一个 2x1 矩阵.
matrix2 = tf.constant([[2.],[2.]])

# 创建一个矩阵乘法 matmul op , 把 'matrix1' 和 'matrix2' 作为输入.
# 返回值 'product' 代表矩阵乘法的结果.
product = tf.matmul(matrix1, matrix2)
```



#### 交互式使用

文档中的 Python 示例使用一个会话 [`Session`](http://www.tensorfly.cn/tfdoc/api_docs/python/client.html#Session) 来 启动图, 并调用 [`Session.run()`](http://www.tensorfly.cn/tfdoc/api_docs/python/client.html#Session.run) 方法执行操作.

为了便于使用诸如 [IPython](http://ipython.org/) 之类的 Python 交互环境, 可以使用 [`InteractiveSession`](http://www.tensorfly.cn/tfdoc/api_docs/python/client.html#InteractiveSession) 代替 `Session` 类, **使用 `Tensor.eval()`和 `Operation.run()`** 方法代替 `Session.run()`. 这样可以避免使用一个变量来持有会话.

```python
# 进入一个交互式 TensorFlow 会话.
import tensorflow as tf
sess = tf.InteractiveSession()

x = tf.Variable([1.0, 2.0])
a = tf.constant([3.0, 3.0])

# 使用初始化器 initializer op 的 run() 方法初始化 'x' 
x.initializer.run()

# 增加一个减法 sub op, 从 'x' 减去 'a'. 运行减法 op, 输出结果 
sub = tf.sub(x, a)
print sub.eval()
# ==> [-2. -1.]
```



### 由另一个变量初始化

你有时候会需要用另一个变量的初始化值给当前变量初始化。由于`tf.initialize_all_variables()`是并行地初始化所有变量，所以在有这种需求的情况下需要小心。

用其它变量的值初始化一个新的变量时，使用其它变量的`initialized_value()`属性。你可以直接把已初始化的值作为新变量的初始值，或者把它当做tensor计算得到一个值赋予新变量。

```python
# Create a variable with a random value.
weights = tf.Variable(tf.random_normal([784, 200], stddev=0.35),
                      name="weights")
# Create another variable with the same value as 'weights'.
w2 = tf.Variable(weights.initialized_value(), name="w2")
# Create another variable with twice the value of 'weights'
w_twice = tf.Variable(weights.initialized_value() * 0.2, name="w_twice")
```

### TensorBoard:可视化学习　

你可能希望记录学习速度(learning rate)的如何变化，以及目标函数如何变化。通过**添加scalar_summary操作**来分别输出学习速度和期望误差。

在TensorFlow中，**所有的操作只有当你执行，或者另一个操作依赖于它的输出时才会运行**。我们刚才创建的这些节点summary nodes都围绕着你的图像：没有任何操作依赖于它们的结果。因此，为了生成汇总信息，我们需要运行所有这些节点。这样的手动工作是很乏味的，因此可以使用[tf.merge_all_summaries](http://www.tensorfly.cn/tfdoc/api_docs/python/train.html#scalary_summary)来将他们合并为一个操作。

然后你可以执行合并命令，它会依据特点步骤将所有数据生成一个序列化的`Summary` protobuf对象。最后，为了将汇总数据写入磁盘

#### 连接关系

TensorFlow 图表有两种连接关系：**数据依赖和控制依赖**。数据依赖显示两个操作之间的tensor流程，用实心箭头指示，而控制依赖用点线表示。

| 结构视图：（颜色相同表示相同结构）                        | 设备视图：（CPU与GPU颜色不同）                       |
| ---------------------------------------- | ---------------------------------------- |
| ![colorby_structure](E:\桌面\临时笔记（每周整理）\tensflow\官方教程记录\colorby_structure.png) | ![colorby_device](E:\桌面\临时笔记（每周整理）\tensflow\官方教程记录\colorby_device.png) |
| 灰色节点的结构是唯一的。橙色的`conv1`和`conv2`节点有相同的结构, 其他颜色的节点也类似。 | 名称域根据其中的操作节点的设备片件来着色，在此紫色代表GPU，绿色代表CPU。  |



### 变量共享

#### tf.variable_scope()基础

当前变量作用域可以用`tf.get_variable_scope()`进行检索，可以通过调用`tf.get_variable_scope().reuse_variables()`将`reuse` 标签设置为`True` .

```
with tf.variable_scope("foo"):
    v = tf.get_variable("v", [1])
    tf.get_variable_scope().reuse_variables()
    v1 = tf.get_variable("v", [1])
assert v1 == v

```

注意**你*不能*设置`reuse`标签为`False`.**其中的原因就是允许改写创建模块的方法.想一下你前面写得方法`my_image_filter(inputs)`.有人在变量作用域内调用`reuse=True` 是希望所有内部变量都被重用.如果允许在方法体内强制执行`reuse=False`，将会打破内部结构并且用这种方法使得很难再共享参数.

即使你不能直接设置 `reuse` 为 `False` ,但是你可以输入一个重用变量作用域,然后就释放掉,就成为非重用的变量.当打开一个变量作用域时,使用`reuse=True` 作为参数是可以的.但也要注意，同一个原因，`reuse`参数是不可继承.所以当你打开一个重用变量作用域，那么所有的子作用域也将会被重用.

```python
with tf.variable_scope("root"):
    # At start, the scope is not reusing.
    assert tf.get_variable_scope().reuse == False
    with tf.variable_scope("foo"):
        # Opened a sub-scope, still not reusing.
        assert tf.get_variable_scope().reuse == False
    with tf.variable_scope("foo", reuse=True):
        # Explicitly opened a reusing scope.
        assert tf.get_variable_scope().reuse == True
        with tf.variable_scope("bar"):
            # Now sub-scope inherits the reuse flag. 子作用域也将会被重用
            assert tf.get_variable_scope().reuse == True
    # Exited the reusing scope, back to a non-reusing one.
    assert tf.get_variable_scope().reuse == False
```



#### 获取变量作用域

在上面的所有例子中，我们共享参数只因为他们的名字是一致的，那是因为我们开启一个变量作用域重用时刚好用了同一个字符串.在更复杂的情况，他可以通过变量作用域对象来使用，而不是通过依赖于右边的名字来使用.为此,变量作用域可以被获取并使用，而不是仅作为当开启一个新的变量作用域的名字.

```python
with tf.variable_scope("foo") as foo_scope:
    v = tf.get_variable("v", [1])
with tf.variable_scope(foo_scope)
    w = tf.get_variable("w", [1])
with tf.variable_scope(foo_scope, reuse=True)
    v1 = tf.get_variable("v", [1])
    w1 = tf.get_variable("w", [1])
assert v1 == v
assert w1 == w
```

当开启一个变量作用域，使用一个预先已经存在的作用域时，我们会跳过当前变量作用域的前缀而直接成为一个完全不同的作用域.这就是我们做得完全独立的地方.

```python
with tf.variable_scope("foo") as foo_scope:
    assert foo_scope.name == "foo"
with tf.variable_scope("bar")
    with tf.variable_scope("baz") as other_scope:
        assert other_scope.name == "bar/baz"
        with tf.variable_scope(foo_scope) as foo_scope2:
            assert foo_scope2.name == "foo"  # Not changed. 注意这里的作用域名是顶级的
```

#### 变量作用域（variable_scope）中的初始化器initializer

使用`tf.get_variable()`允许你重写方法来创建或者重用变量,并且可以被外部透明调用

但是如果我们想改变创建变量的初始化器那要怎么做呢？是否我们需要为所有的创建变量方法传递一个额外的参数呢？那在大多数情况下，**当我们想在一个地方并且为所有的方法的所有的变量设置一个默认初始化器，那又改怎么做呢？**为了解决这些问题，**变量作用域可以携带一个默认的初始化器.他可以被子作用域继承并传递给`tf.get_variable()` 调用**.但是如果其他初始化器被明确地指定,那么他将会被覆盖.

```python
with tf.variable_scope("foo", initializer=tf.constant_initializer(0.4)):
    v = tf.get_variable("v", [1])
    assert v.eval() == 0.4  # 使用上面默认的初始化器.
    
    w = tf.get_variable("w", [1], initializer=tf.constant_initializer(0.3)):
    assert w.eval() == 0.3  # 使用指定的初始化器，覆盖默认的.
    
    with tf.variable_scope("bar"):
        v = tf.get_variable("v", [1])
        assert v.eval() == 0.4  # 子作用域继承自默认的初始化器.
        
    with tf.variable_scope("baz", initializer=tf.constant_initializer(0.2)):
        v = tf.get_variable("v", [1])
        assert v.eval() == 0.2  # 修改默认的初始化器.
```



#### 在`tf.variable_scope()`中操作结点（ops）的名称

我们讨论 `tf.variable_scope` 怎么处理变量的名字.但是又是如何在作用域中影响到 其他ops的名字的呢？ops在一个变量作用域的内部创建，那么他应该是共享他的名字，这是很自然的想法.出于这样的原因，当我们用`with tf.variable_scope("name")`时，这就间接地开启了一个`tf.name_scope("name")`.比如：

```python
with tf.variable_scope("foo"):
    x = 1.0 + tf.get_variable("v", [1])
assert x.op.name == "foo/add"
```

名称作用域可以被开启并添加到一个变量作用域中,然后他们**只会影响到ops的名称,而不会影响到变量.**

```python
with tf.variable_scope("foo"):
    with tf.name_scope("bar"):
        #  不会影响到变量v，只会影响到ops的名称，比如下面的加法操作
        v = tf.get_variable("v", [1])
        x = 1.0 + v
assert v.name == "foo/v:0"
assert x.op.name == "foo/bar/add"
```

**当用一个引用对象而不是一个字符串去开启一个变量作用域时，我们就不会为ops改变当前的名称作用域.**



